{
  "activeProvider": "claude",
  "usePluginSystem": true,
  "providers": [
    {
      "provider": "mock",
      "name": "Mock AI Provider",
      "enabled": true,
      "priority": 100,
      "options": {
        "seed": 12345,
        "scenarios": ["exploration", "combat", "dialogue"]
      }
    },
    {
      "provider": "claude",
      "name": "Claude AI (Anthropic)",
      "enabled": true,
      "priority": 1,
      "model": "claude-sonnet-4-5",
      "apiKey": "YOUR_ANTHROPIC_API_KEY_HERE",
      "options": {
        "fallbackModels": [
          "claude-opus-4-1",
          "claude-opus-4-0",
          "claude-sonnet-4-0"
        ],
        "maxRetries": 3,
        "temperature": 0.7
      }
    },
    {
      "provider": "openai",
      "name": "OpenAI GPT-4",
      "enabled": false,
      "priority": 2,
      "model": "gpt-4o",
      "apiKey": "YOUR_OPENAI_API_KEY_HERE",
      "options": {
        "organization": "org-xxx",
        "project": "proj_xxx",
        "temperature": 0.7,
        "maxTokens": 4096
      }
    },
    {
      "provider": "openai",
      "name": "OpenAI GPT-3.5 (Budget)",
      "enabled": false,
      "priority": 3,
      "model": "gpt-3.5-turbo",
      "apiKey": "YOUR_OPENAI_API_KEY_HERE",
      "options": {
        "temperature": 0.7,
        "maxTokens": 2048
      }
    },
    {
      "provider": "local",
      "name": "Local LLM (Ollama)",
      "enabled": false,
      "priority": 4,
      "model": "llama2",
      "endpoint": "http://localhost:11434/api",
      "options": {
        "temperature": 0.7,
        "contextWindow": 4096
      }
    }
  ],
  "global": {
    "cacheEnabled": true,
    "cacheTTL": 3600,
    "loggingEnabled": true,
    "logLevel": "info",
    "retry": {
      "maxRetries": 3,
      "retryDelay": 1000,
      "backoffMultiplier": 2
    }
  }
}